{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3801d352",
      "metadata": {
        "id": "3801d352"
      },
      "source": [
        "## Домашнее задание : \"Обучение с подкреплением\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c3d037",
      "metadata": {
        "id": "e0c3d037"
      },
      "source": [
        "ФИО: Лыжин Роман Денисович"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9528f5be",
      "metadata": {
        "id": "9528f5be"
      },
      "source": [
        "# Задание 1\n",
        "\n",
        "Обучите алгоритм Q-learning для сред FrozenLake-v1 и Blackjack-v1, в частности подберите оптимальную alpha. (2 балла)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "DZHm3elve0Xf"
      },
      "id": "DZHm3elve0Xf",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, num_episodes, alpha, gamma, epsilon):\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            best_next_action = np.argmax(Q[next_state])\n",
        "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return Q"
      ],
      "metadata": {
        "id": "361NWHR4fAh9"
      },
      "id": "361NWHR4fAh9",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_alpha(env_name, alpha_values, num_episodes=1000, gamma=0.99, epsilon=0.1):\n",
        "    env = gym.make(env_name)\n",
        "    best_alpha = None\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    for alpha in alpha_values:\n",
        "        Q = q_learning(env, num_episodes, alpha, gamma, epsilon)\n",
        "\n",
        "        total_reward = 0\n",
        "        for _ in range(100):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = np.argmax(Q[state])\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "\n",
        "        avg_reward = total_reward / 100\n",
        "        if avg_reward > best_reward:\n",
        "            best_reward = avg_reward\n",
        "            best_alpha = alpha\n",
        "\n",
        "    return best_alpha, best_reward"
      ],
      "metadata": {
        "id": "kliG8XYKfBur"
      },
      "id": "kliG8XYKfBur",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_values = np.linspace(0.01, 1.0, 20)\n",
        "optimal_alpha_frozenlake, reward_frozenlake = optimize_alpha(\"FrozenLake-v1\", alpha_values)"
      ],
      "metadata": {
        "id": "rBt47jpifI8C"
      },
      "id": "rBt47jpifI8C",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_alpha_frozenlake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYAmbWfofMfM",
        "outputId": "9459a051-ef3d-45c5-f952-6be2a18e4477"
      },
      "id": "VYAmbWfofMfM",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7394736842105263"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward_frozenlake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahLZdHf-ioKT",
        "outputId": "a1ae1132-c131-4308-bfc8-95c997db180c"
      },
      "id": "ahLZdHf-ioKT",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_blackjack(env, num_episodes, alpha, gamma, epsilon):\n",
        "    Q = {}\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if state not in Q:\n",
        "                Q[state] = np.zeros(env.action_space.n)\n",
        "\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if next_state not in Q:\n",
        "                Q[next_state] = np.zeros(env.action_space.n)\n",
        "\n",
        "            best_next_action = np.argmax(Q[next_state])\n",
        "            Q[state][action] = Q[state][action] + alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return Q"
      ],
      "metadata": {
        "id": "WH9crJe1ixgL"
      },
      "id": "WH9crJe1ixgL",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_alpha_blackjack(alpha_values, num_episodes=1000, gamma=0.99, epsilon=0.1):\n",
        "    env = gym.make(\"Blackjack-v1\")\n",
        "    best_alpha = None\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    for alpha in alpha_values:\n",
        "        Q = q_learning_blackjack(env, num_episodes, alpha, gamma, epsilon)\n",
        "\n",
        "        total_reward = 0\n",
        "        for _ in range(100):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = np.argmax(Q.get(state, np.zeros(env.action_space.n)))\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "\n",
        "        avg_reward = total_reward / 100\n",
        "        if avg_reward > best_reward:\n",
        "            best_reward = avg_reward\n",
        "            best_alpha = alpha\n",
        "\n",
        "    return best_alpha, best_reward"
      ],
      "metadata": {
        "id": "xt6DPjEqizIu"
      },
      "id": "xt6DPjEqizIu",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_alpha_blackjack, reward_blackjack = optimize_alpha_blackjack(alpha_values)"
      ],
      "metadata": {
        "id": "qaSunPrwi1IZ"
      },
      "id": "qaSunPrwi1IZ",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_alpha_blackjack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1BeQSLpi3oZ",
        "outputId": "20de4de2-ec73-4fb0-942d-eb84ac154705"
      },
      "id": "p1BeQSLpi3oZ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward_blackjack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03gZikeDi4X6",
        "outputId": "67eefa36-37c0-4fc6-c763-32e4ae713ace"
      },
      "id": "03gZikeDi4X6",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89eab25f",
      "metadata": {
        "id": "89eab25f"
      },
      "source": [
        "# Задание 2\n",
        "\n",
        "Обучите алгоритм Policy Gradients (или Actor Critic) для среды https://www.gymlibrary.dev/environments/atari/breakout/ . Продемонстрируйте, что для обученного агента растет время игры. (3 балла)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}